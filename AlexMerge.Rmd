---
title: "AlexMerge"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(foreign)
library(tidyverse)
library(DMwR2)
library(e1071)
library(glmnet)



data_merge1<-read.spss("data/W1 Merged Data/Wave.1_Data/Merge/Wave1_20170906.sav", 
                  to.data.frame=TRUE)

# data_merge1 <- data_merge1%>%
#   select(se002, se005, se009, country, q007, q008, q009, q010, q006, q098, q128, q005, q027, q105, q106, q123, q127)
data_merge1 <- data_merge1%>%
   select(se002, se005, se009, country, q027, q105, q106)



#data_merge1<- knnImputation(data_merge1, k = 10)



#data_merge2<-read.spss("data/W2 Merged Data/2w-3rd_release_all/merge/Wave2_20170724.sav", 
                  #to.data.frame=TRUE)
#data_merge2

#tune.out <- tune(svm, q106 ~ ., data = data_merge1, kernel = "linear",
                 #ranges = list(cost = c(0.001, 0.01, 0.1,
                  #                      1, 5, 10, 100)))
#summary(tune.out)


```

```{r}
# data_merge1 <- na.omit(data_merge1)
# data_merge1
# # data_merge1 <- data_merge1%>% 
# #   mutate(q106 = case_when(q106=="Much worse" ~ 0,
# #                           q106=="Somewhat worse" ~ 1,
# #                           q106=="Much the same" ~ 2,
# #                           q106=="Somewhat better" ~ 3,
# #                           q106=="Much better that before" ~ 4,))
# x <- model.matrix(q106~.,data_merge1)
# x
# y <- data_merge1$q106
# #Finding optimal lambda shrinkage value to reduce error through cross validation
# set.seed(123)
# cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "multinomial")
# #Creation of lasso model and outputting regression coefficients
# lasso_model <- glmnet(x, y, alpha = 1, family = "multinomial",
# lambda = cv.lasso$lambda.min)
# coef(lasso_model)

```
```{r}
tune.out1 <- tune(svm, q106 ~ ., data = data_merge1, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1,
                                        1, 5, 10, 100)))
summary(tune.out1)

```

```{r}

data_merge1 <- data_merge1%>% 
  drop_na() 
data_merge1 %>%
  count(country)

data_merge1 <- data_merge1%>% 
   mutate(q106 = case_when(q106=="Much worse" ~ 0,
                           q106=="Somewhat worse" ~ 0,
                           q106=="Much the same" ~ 0,
                           q106=="Somewhat better" ~ 1,
                           q106=="Much better than before" ~ 1,))
x <- model.matrix(q106~.,data_merge1)[,-1]
x
y <- data_merge1$q106
#Finding optimal lambda shrinkage value to reduce error through cross validation
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
#Creation of lasso model and outputting regression coefficients
lasso_model <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
coef(lasso_model)
cv.lasso$lambda.min
```
We decided to transform our response variable q106 into two different categories. "Much worse," "Somewhat worse," "Much the same," were encoded as 0, and "Somewhat better, "Much better than before" were encoded as 1. As a result, our goal is to predict if trends in the equal treatment of citizens has gotten worse/stayed the same, or gotten better. We decided to drop all NA's from the dataframe. We considered using KNN to impute missing values, but we thought the missing data may have a systematic pattern, as seen in the GRAPHHHHH. We then decided to fit a lasso regression in order to decrease the variance of our model and also perform variable selection. 

